{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RTS_lrGZZdmk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTS_lrGZZdmk",
    "outputId": "e9f74a00-e977-4969-f0f6-89ed379b23d2"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade ultralytics==8.0.186\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install opencv-python-headless\n",
    "!pip install albumentations==1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0430c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e0430c4",
    "outputId": "1b183851-cd58-4e8b-9192-fbf82f0c110d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import os\n",
    "import albumentations\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605a2d3",
   "metadata": {
    "id": "8605a2d3"
   },
   "outputs": [],
   "source": [
    "dataset_yolo = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Person-Detection-YOLOV8N-Dectection'\n",
    "epoch_number = 200\n",
    "weights_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/YoloV8/yolov8n.pt'\n",
    "data_yaml_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Person-Detection-YOLOV8N-Dectection/data.yaml'\n",
    "saved_path = f'trained_model'\n",
    "results_folder_name = f\"{dataset_yolo}_normal\"\n",
    "\n",
    "# model = YOLO(weights_path)\n",
    "# model.train(\n",
    "#     data=data_yaml_path,\n",
    "#     epochs=epoch_number,\n",
    "#     batch=32,\n",
    "#     device='cuda',\n",
    "#     project=saved_path,\n",
    "#     name=results_folder_name,\n",
    "#     patience=epoch_number,\n",
    "#     pretrained=True,\n",
    "#     lr0=0.001,\n",
    "#     lrf=0.001,\n",
    "#     dropout=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXQmnAP8BCbJ",
   "metadata": {
    "id": "RXQmnAP8BCbJ"
   },
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, epsilon, alpha, num_iter, target_class=0):\n",
    "\n",
    "    images_adv = images.clone().detach().requires_grad_(True)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        for i in range(num_iter):\n",
    "\n",
    "            images_adv.retain_grad()\n",
    "            outputs = model(images_adv)\n",
    "\n",
    "            confidences = outputs[0].boxes.conf \n",
    "            if target_class is not None:\n",
    "                classes = outputs[0].boxes.cls\n",
    "                target_conf = confidences[classes == target_class]\n",
    "                if len(target_conf) == 0: \n",
    "                    continue\n",
    "                Closs = -torch.mean(target_conf) \n",
    "            else:\n",
    "                Closs = -torch.mean(confidences)  \n",
    "\n",
    "            # loss = -images_adv.mean()\n",
    "            loss = images_adv.mean() - Closs\n",
    "\n",
    "            print(loss)\n",
    "\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            if images_adv.grad is None:\n",
    "                print(\"images_adv.grad is None after backward\")\n",
    "                break\n",
    "\n",
    "            grad_sign = images_adv.grad.sign()\n",
    "\n",
    "            images_adv = images_adv + alpha * grad_sign\n",
    "\n",
    "            eta = torch.clamp(images_adv - images, min=-epsilon, max=epsilon)\n",
    "            images_adv = torch.clamp(images + eta, min=0, max=3).detach()\n",
    "\n",
    "            images_adv.requires_grad_(True)\n",
    "\n",
    "    return images_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8_NMdev2OEpQ",
   "metadata": {
    "id": "8_NMdev2OEpQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, size=(640, 640)):\n",
    "\n",
    "    image_resized = cv2.resize(image, size)\n",
    "    image_tensor = torch.tensor(image_resized, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "    return image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJoUmOcUDFcJ",
   "metadata": {
    "id": "fJoUmOcUDFcJ"
   },
   "outputs": [],
   "source": [
    "def draw_boxes(image, outputs, class_names=None, color=(0, 255, 0)):\n",
    "\n",
    "    image_with_boxes = image.copy()\n",
    "\n",
    "    boxes = outputs[0].boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist()) \n",
    "        conf = float(box.conf[0])\n",
    "        cls = int(box.cls[0]) \n",
    "\n",
    "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), color, 2)\n",
    "        print(f\"{conf:.2f}\")\n",
    "\n",
    "        label = f\"{class_names[cls] if class_names else cls}: {conf:.2f}\"\n",
    "        cv2.putText(image_with_boxes, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image_with_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udoOuInFK8zH",
   "metadata": {
    "id": "udoOuInFK8zH"
   },
   "outputs": [],
   "source": [
    "def attack_and_show_image(model, image_path, epsilon, alpha, num_iter, class_range, size=(640, 640)):\n",
    "  image = cv2.imread(image_path)\n",
    "  outputs = model(image)\n",
    "\n",
    "  # images = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "  # labels = torch.tensor([0]).to(device)\n",
    "  images = preprocess_image(image, size=(640, 640))\n",
    "\n",
    "  print(\"Generating adversarial examples...\")\n",
    "  images_adv = pgd_attack(model, images, epsilon, alpha, num_iter)\n",
    "\n",
    "  outputs_adv = model(images_adv)\n",
    "\n",
    "  image_with_boxes = draw_boxes(image, outputs, class_range)\n",
    "\n",
    "  plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "\n",
    "  adv_img_np = images_adv.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "  adv_img_np = (adv_img_np * 255).astype('uint8')\n",
    "\n",
    "  image_with_boxes = draw_boxes(adv_img_np, outputs_adv, class_range)\n",
    "\n",
    "  plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AU-9NOztNu5z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AU-9NOztNu5z",
    "outputId": "d1c61888-7597-419e-dcea-34630f284db4"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = f\"{results_folder_name}/weights/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "epsilon = 0.03 \n",
    "alpha = 0.001  \n",
    "num_iter = 10  \n",
    "class_range = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qw32CxwSJD0I",
   "metadata": {
    "id": "Qw32CxwSJD0I"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # 禁用参数的更新\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahagtP5RMWvT",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ahagtP5RMWvT",
    "outputId": "1918a98d-774f-458b-9c1f-3b8b261ac316"
   },
   "outputs": [],
   "source": [
    "folder_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Person-Detection-YOLOV8N-Dectection/test/images'\n",
    "image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.jpg')]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    attack_and_show_image(model, image_path, epsilon, alpha, num_iter, class_range, size=(640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PFME0LYQPC1f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PFME0LYQPC1f",
    "outputId": "ec4ff636-811c-4108-bacf-f7c342a1f5d7"
   },
   "outputs": [],
   "source": [
    "folder_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Person-Detection-YOLOV8N-Dectection/valid/images'\n",
    "image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.jpg')]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    attack_and_show_image(model, image_path, epsilon, alpha, num_iter, class_range, size=(640, 640))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
