{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RTS_lrGZZdmk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTS_lrGZZdmk",
    "outputId": "f2697700-19f1-4593-9e85-96cfa2bc2399"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade ultralytics==8.0.186\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install opencv-python-headless\n",
    "!pip install albumentations==1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0430c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e0430c4",
    "outputId": "62e9a646-16ae-4ae0-baf7-c84c7f9ce9d1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import os\n",
    "import albumentations\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605a2d3",
   "metadata": {
    "id": "8605a2d3"
   },
   "outputs": [],
   "source": [
    "dataset_yolo = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Senior-Design-VIAD'\n",
    "epoch_number = 50\n",
    "weights_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/YoloV8/yolov8n.pt'\n",
    "data_yaml_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Senior-Design-VIAD/data.yaml'\n",
    "saved_path = f'trained_model'\n",
    "results_folder_name = f\"{dataset_yolo}_normal\"\n",
    "\n",
    "# model = YOLO(weights_path)\n",
    "# model.train(\n",
    "#     data=data_yaml_path,\n",
    "#     epochs=epoch_number,\n",
    "#     batch=40,\n",
    "#     device='cuda',\n",
    "#     project=saved_path,\n",
    "#     name=results_folder_name,\n",
    "#     patience=epoch_number,\n",
    "#     pretrained=True,\n",
    "#     lr0=0.01,\n",
    "#     lrf=0.001,\n",
    "#     dropout=0.2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auCG1JfVjBzD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auCG1JfVjBzD",
    "outputId": "31f53224-8c15-4504-f789-d971d31aeef4"
   },
   "outputs": [],
   "source": [
    "data_yaml_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Senior-Design-VIAD/data.yaml'\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    print(f.read())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXQmnAP8BCbJ",
   "metadata": {
    "id": "RXQmnAP8BCbJ"
   },
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, epsilon, alpha, num_iter, target_class=13, lambda1=1.0, lambda2=1.0):\n",
    "    images_adv = images.clone().detach().requires_grad_(True)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        for i in range(num_iter):\n",
    "            images_adv.retain_grad()\n",
    "            outputs = model(images_adv)\n",
    "\n",
    "            if outputs[0].boxes is None:\n",
    "                # print(f\"Iteration {i+1}: No objects detected, skipping.\")\n",
    "                continue\n",
    "\n",
    "            classes = outputs[0].boxes.cls \n",
    "            confidences = outputs[0].boxes.conf  \n",
    "\n",
    "            target_conf = confidences[classes == target_class]\n",
    "            other_conf = confidences[classes != target_class]\n",
    "\n",
    "            if len(target_conf) == 0 and len(other_conf) == 0:\n",
    "                # print(f\"Iteration {i+1}: No objects detected, skipping.\")\n",
    "                continue\n",
    "\n",
    "            mean_pixel_loss = images_adv.mean() \n",
    "            confidence_loss = -lambda1 * torch.mean(target_conf) if len(target_conf) > 0 else 0\n",
    "            suppression_loss = lambda2 * torch.mean(1 - other_conf) if len(other_conf) > 0 else 0\n",
    "            loss = mean_pixel_loss + confidence_loss + suppression_loss\n",
    "\n",
    "            # print(f\"Iteration {i+1}/{num_iter}, Loss: {loss.item()}\")\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            if images_adv.grad is None:\n",
    "                # print(\"images_adv.grad is None after backward\")\n",
    "                break\n",
    "\n",
    "            grad_sign = images_adv.grad.sign()\n",
    "\n",
    "            images_adv = images_adv + alpha * grad_sign\n",
    "\n",
    "            eta = torch.clamp(images_adv - images, min=-epsilon, max=epsilon)\n",
    "            images_adv = torch.clamp(images + eta, min=0, max=3).detach()\n",
    "\n",
    "            images_adv.requires_grad_(True)\n",
    "\n",
    "    return images_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fJoUmOcUDFcJ",
   "metadata": {
    "id": "fJoUmOcUDFcJ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def draw_boxes(image, outputs, class_names=None, color=(0, 255, 0)):\n",
    "\n",
    "    image_with_boxes = image.copy()\n",
    "\n",
    "    boxes = outputs[0].boxes  \n",
    "\n",
    "    if boxes is None:\n",
    "        print(\"No objects detected.\")\n",
    "        return image_with_boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist()) \n",
    "        conf = float(box.conf[0]) \n",
    "        cls = int(box.cls[0])  \n",
    "\n",
    "        if int(cls)==13:\n",
    "          color=(0, 255, 0)\n",
    "        else:\n",
    "          color = (int(cls)*10, int(cls)*10, int(cls)*10)\n",
    "\n",
    "        if class_names and cls < len(class_names):\n",
    "            label = f\"{class_names[cls]}: {conf:.2f}\"\n",
    "        else:\n",
    "            label = f\"Class {cls}: {conf:.2f}\" \n",
    "\n",
    "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), color, 2)\n",
    "        print(f\"{conf:.2f}\")\n",
    "\n",
    "        label = f\"{class_names[cls] if class_names else cls}: {conf:.2f}\"\n",
    "        cv2.putText(image_with_boxes, label, (x1 + 10, y1 + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image_with_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ANVYf46Bfw3e",
   "metadata": {
    "id": "ANVYf46Bfw3e"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, size=(640, 640)):\n",
    "\n",
    "    image_resized = cv2.resize(image, size)\n",
    "    image_tensor = torch.tensor(image_resized, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "    return image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p7lfLR9yRYjF",
   "metadata": {
    "id": "p7lfLR9yRYjF"
   },
   "outputs": [],
   "source": [
    "def attack_and_show_image(model, image_path, epsilon, alpha, num_iter, class_range, size=(640, 640)):\n",
    "  image = cv2.imread(image_path)\n",
    "  outputs = model(image)\n",
    "\n",
    "  # images = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "  # labels = torch.tensor([0]).to(device)\n",
    "  images = preprocess_image(image, size=(640, 640))\n",
    "\n",
    "  print(\"Generating adversarial examples...\")\n",
    "  images_adv = pgd_attack(model, images, epsilon, alpha, num_iter)\n",
    "\n",
    "  outputs_adv = model(images_adv)\n",
    "\n",
    "  image_with_boxes = draw_boxes(image, outputs, class_range)\n",
    "\n",
    "  plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "\n",
    "  adv_img_np = images_adv.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "  adv_img_np = (adv_img_np * 255).astype('uint8')\n",
    "\n",
    "  image_with_boxes = draw_boxes(adv_img_np, outputs_adv, class_range)\n",
    "\n",
    "  plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w39IiKQDDeDg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w39IiKQDDeDg",
    "outputId": "153dbcb0-d599-4cc6-ac17-dd13382c4f37"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = f\"{results_folder_name}/weights/last.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "class_range = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "\n",
    "epsilon = 0.05  \n",
    "alpha = 0.001  \n",
    "num_iter = 100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3pF7SZGRouY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G3pF7SZGRouY",
    "outputId": "a0d8963a-8968-44fe-d87f-4f3f1dc19199"
   },
   "outputs": [],
   "source": [
    "folder_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Person-Detection-YOLOV8N-Dectection/valid/images'\n",
    "image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.jpg')]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    attack_and_show_image(model, image_path, epsilon, alpha, num_iter, class_range, size=(640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iQUhrzLiRphM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iQUhrzLiRphM",
    "outputId": "3dfa57a7-8765-40cc-bb67-4a07f6fae9bf"
   },
   "outputs": [],
   "source": [
    "folder_path = '/content/drive/My Drive/2024 Fall/COMS 6730/Projects/Person-Detection-YOLOV8N-Dectection/test/images'\n",
    "image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.jpg')]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    attack_and_show_image(model, image_path, epsilon, alpha, num_iter, class_range, size=(640, 640))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
